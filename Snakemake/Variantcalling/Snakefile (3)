import subprocess
from scipy import stats
import pandas as pd
import numpy as np
import os, yaml
import pysam
from matplotlib import pyplot as plt
jn = os.path.join

TEST = False
N_TEST_CHUNKS = 1
START = int(5e6)
END = int(5e6 + 1e5)

## Config
configfile: "config.yaml"
ana_dir = config['ana_dir']
alignment_dir = config['alignment_dir']

################### Global parameters ############################



#dtype=str is important for correct functioning!

#This is important, because otherwise the wildcards in the rules are ambiguous
wildcard_constraints:
    i="\d+",
    contig="\d+",
    chrom="\w+",
    ref_fn=".*.fa",
    #sequence_id="\w+",
    ind_filter_id="[^.]+",
    site_filter_id="[^.]+",
    family_id="\w+",
    filter_id="[^.]+",#"^/(?!raw|all_sites)([^.]+)$"
    vcf_type="(pass.snps.biallelic|pass.indels.biallelic|variants|NoSFilter.snps.biallelic)",
    vcf_extension="(vcf|vcf.gz|bcf)",
    chunk="\d+",
            #"[^.]+",



sample_mt = pd.read_csv(config["sample_mt"], dtype=str, sep='\t', index_col=0)#.set_index("sequence_id", drop=False)

sample_mt.drop_duplicates(inplace=True)

samples = sample_mt.index.values

studies = list(sample_mt.loc[samples,'study'])

n_samples = len(sample_mt)

chrom_length = pd.read_csv(config['ref']['base_fn'] + config['ref']['ext_fai'],
                           sep='\t',usecols=[0,1],names=['chrom','len'],index_col=0,
                           squeeze=True)

max_chrom_length = chrom_length.max()

chunk_size = 5e6 #define chunk size here 

chunk_to_region = {i:(int(i*chunk_size+1),
                      int((i+1)*chunk_size)) for i in range(int(np.ceil(max_chrom_length/chunk_size)))}


ref_name = config['ref']['name']
ref_base = config['ref']['base_fn']
ref_ext = config['ref']['ext_fa']
ref_indexes = ['.amb', '.ann', '.bwt', '.pac','.sa','.fai']

CHROMOSOMES = [f'chr{i}' for i in range(1,24) if i!= 21]
CHUNKS = range(len(chunk_to_region))

ref_name = config['ref']['name']
callset_id = config['callset_id']
ind_filter_id ='tif1'
site_filter_id = 'sft1'
filter_id = 'bla'
vcf_type = 'pass.snps.biallelic'
vcf_extension = 'vcf.gz'



def get_chunks(chrom):
    chunks = range(int(np.ceil(chrom_length[chrom] / chunk_size)))
    if TEST:
        mid_chunk = len(chunks) // 2
        chunks = [c for c in chunks][mid_chunk:mid_chunk + N_TEST_CHUNKS]
    return  chunks


######################### rule all ##################################
rule all:
    input:
        #f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.wg.bcf'
        #expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.{{chrom}}.vcf.gz', chrom = CHROMOSOMES)
        #expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{{chrom}}.{{sample_id}}.bcf.csi', chrom = CHROMOSOMES, sample_id = samples)
#####Alignment
        #expand(f"{ana_dir}/_data/Alignment/{ref_name}/{{sequence_id}}.fixmate.sort.markdup.rg.bam", sequence_id=samples)
#####flagstat
        #expand(f"{ana_dir}/_data/Alignment/{ref_name}/{{sequence_id}}.fixmate.sort.markdup.rg.bam.flagstat", sequence_id=samples)
# ####Bam to crumble.cram
#             expand(f"{alignment_dir}/{{study_name}}/{{sequence_id}}/{ref_name}/{{sequence_id}}.mem.crumble.cram", zip, study_name = studies, sequence_id = samples),
# ####Index crumble.cram
#             expand(f"{alignment_dir}/{{study_name}}/{{sequence_id}}/{ref_name}/{{sequence_id}}.mem.crumble.cram.crai", zip, study_name = studies, sequence_id = samples)
######Variant_calling
         #expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.variants.raw.prefilter.{{chrom}}.bcf", chrom=CHROMOSOMES,sequence_id=samples),
#####All_sites_calling
         #expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.all_sites_{{chrom}}.bcf", chrom=CHROMOSOMES,sequence_id=samples),
         #expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.het_test_{{chrom}}.txt.gz",chrom=CHROMOSOMES,sequence_id=samples),
#           expand(f"{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites_{{chrom}}.{{chunk}}.bcf",chunk = CHUNKS, chrom = CHROMOSOMES),
#           expand(f"{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/total_coverage_{{chrom}}.{{chunk}}.txt", chrom = CHROMOSOMES,chunk = CHUNKS)
#####Plot_stats
#         ana_dir + '/Figures/' + f'filter_per_sample.{ref_name}.{callset_id}.{ind_filter_id}.pdf',
#         ana_dir + '/Figures/' + f'stat_hists.{ref_name}.{callset_id}.{ind_filter_id}.{site_filter_id}.pdf',
        expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{{chrom}}.filters.bed.gz', chrom = CHROMOSOMES),
#         expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{{chrom}}.{{chunk}}.filters.info', chrom = CHROMOSOMES, chunk=CHUNKS)
        #         expand(f"{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{{chrom}}.{{chunk}}.bcf", chrom=CHROMOSOMES ,chunk = CHUNKS)
# print(ana_dir + '/Figures/' + f'filter_per_sample.{ref_name}.{callset_id}.{ind_filter_id}.pdf')
####bialellic & filtering
####probably run this as rule all often.
#             expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.{{chrom}}.bcf', chrom=CHROMOSOMES)
# ####shapeit
#              expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.{{chrom}}.vcf.gz', chrom = CHROMOSOMES)
#              expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.{{chrom}}.vcf.gz', chrom = CHROMOSOMES) #annotate.{{chrom}}.vcf.gz
#              expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{{chrom}}_.bcf',chrom = CHROMOSOMES),#for all unfiltered
#              expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{{chrom}}.{{sample_id}}.bcf.csi', chrom = CHROMOSOMES, sample_id = samples)
#              f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.wg.bcf'
#######add_ancest_seq
#               expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.anc_samp.{{chrom}}.{vcf_extension}', chrom = CHROMOSOMES),
#               expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.anc_samp.{{chrom}}.{vcf_extension}.tbi', chrom = CHROMOSOMES),
#               expand(f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.anc_samp.{{chrom}}.{vcf_extension}.csi', chrom = CHROMOSOMES)
              #
########################### fastqc ################################

rule fastqc:
    output:
        f"{ana_dir}/data/reads/fastqc/{{sample}}_fastqc.zip",
        f"{ana_dir}/data/reads/fastqc/{{sample}}_fastqc.html"
    params:
        output_dir = f"{ana_dir}/data/reads/fastqc/"
    run:
        if os.path.isfile(f"{ana_dir}/data/reads/{wildcards.sample}.fq.gz"):
            input = f"{ana_dir}/data/reads/{wildcards.sample}.fq.gz"
        else:
            input = f"{ana_dir}/data/reads/{wildcards.sample}.fastq.gz"   
        shell(f"fastqc {input} --outdir {params.output_dir}")


########################### bwa_index ################################

rule bwa_index:
    input:
        ref = f"{ref_base}.fa"
    output:
        amb = f"{ref_base}.fa.amb",
        ann = f"{ref_base}.fa.ann",
        bwt = f"{ref_base}.fa.bwt",
        pac = f"{ref_base}.fa.pac",
        sa = f"{ref_base}.fa.sa",
        fai = f"{ref_base}.fa.fai",
        dict = f"{ref_base}.dict"
    params:
        ref_base = f"{ref_base}"
    shell:
        """
        samtools faidx {input.ref}
        bwa index {input.ref}
        samtools dict \
            -a {params.ref_base} \
            {input.ref} \
            -o {output.dict}
        """


########################## align reads #################################

def get_read_fn(wildcards):
    file_str = sample_mt.loc[wildcards.sequence_id,'calcua_read_location']
    file_list = file_str.rsplit(',')
    return file_list


rule align_reads:
     input:
         reads = get_read_fn,
         ref = f"{ref_base}.fa",
     output:
         bam = "{ana_dir}/_data/Alignment/{ref_name}/{sequence_id}.fixmate.sort.markdup.rg.bam"
     threads: 7 #12
     resources:
         mem_gb=14,#lambda wildcards, threads: threads*2,
         walltime = 48/
     params:
         add_threads = lambda wildcards, threads: threads-1,
         output_bam_prefix = "{ana_dir}/_data/Alignment/{ref_name}/{sequence_id}",
         ext = lambda wildcards: sample_mt.loc[wildcards['sequence_id'],'calcua_read_location'].rsplit('.',1)[-1],
         rg = '"@RG\\tID:{sequence_id}\\tSM:{sequence_id}\\tPL:ILLUMINA"'
     run:
         s = sample_mt.loc[wildcards['sequence_id']]
         read_files = s['calcua_read_location'].split(',')
         if len(read_files) > 2:
             read_files1 = ' '.join(i for i in read_files if '1.fastq' in i)
             read_files2 = ' '.join(i for i in read_files if '2.fastq' in i)
             read_files = [read_files1, read_files2]
         # don't use -M in bwa mem!!
         if s['read_type'] == 'cram':
             c0 = 'samtools fastq -F 0x200 {input.reads} | bwa mem -t {threads} -R {params.rg} -p {input.ref} - '
         elif s['read_type']  == 'fqz_1_2':
             ext = s['calcua_read_location'].rsplit('.',1)[-1]
             if ext=='gz':
                 ##HS: replaced SRR by [SED]R[A-Z] to make it more general
                 c0 = 'bwa mem -t {threads} -R {params.rg} {input.ref} ' + " ".join(['<(gzip -dc ' + i + ' | sed -E "s/^((@|\+)[SED]R[A-Z][^.]+\.[^.]+)\.(1|2)/\\1/")' for i in read_files])
             else:
                 c0 = 'bwa mem -t {threads} -R {params.rg} {input.ref} ' + ' '.join(i for i in read_files)     
         else:
             raise ValueError("{} Sample_mt read_type must be in: [cram, bam, fastq_pe, fastq_single, fastq_intervleaved, fastq].".format(wildcards['sequence_id']))
         c1 = (' | samtools fixmate --threads {params.add_threads} -m - - '
            ' | samtools sort -T {params.output_bam_prefix}.sort.tmp --threads {params.add_threads} - '
            ' | samtools markdup -T {params.output_bam_prefix}.markdup.tmp --threads {params.add_threads} - {output.bam}; ')
         c2 = 'samtools index {output.bam}'
         print(c0)
         shell(c0 + c1 + c2)


###################### bam statistics #####################################


rule flagstat:
    input:
        bam = f"{ana_dir}/_data/Alignment/{ref_name}/{{sequence_id}}.fixmate.sort.markdup.rg.bam",
        #cram = [sample_mt['calcua_alignment_location_fAstCal1.2'].loc[i] for i in samples if str(sample_mt['calcua_alignment_location_fAstCal1.2'].loc[i]) != 'nan'],
    output:
        flagstat = f"{ana_dir}/_data/Alignment/{ref_name}/{{sequence_id}}.fixmate.sort.markdup.rg.bam.flagstat"
    resources:
        mem_gb=1,
        walltime=1
    shell:
        'samtools flagstat {input.bam} > {output.flagstat}'

###################### crumble.cram ###################### 

rule bam_to_cram:
    input:
        bam = f"{ana_dir}/_data/Alignment/{ref_name}/{{sequence_id}}.fixmate.sort.markdup.rg_TEST.bam",
        ref = f"{ref_base}.fa",
    output:
        CC = f"{alignment_dir}/{{study_name}}/{{sequence_id}}/{ref_name}/{{sequence_id}}.mem_TEST.crumble.cram",
    threads: 1
    resources:
        mem_gb= lambda wildcards, threads: threads*2,
        walltime = 24
    shell:
        """
        /data/antwerpen/grp/asvardal/software/crumble/crumble -O cram,reference={input.ref},lossy_names {input.bam} {output.CC}
        samtools index {output.CC}
        samtools quickcheck -v {output.CC}
        rm {input.bam}
        """

        
###################### calling all sites ######################         
        
rule call_all_sites_vcf:
    input:
        #crams = sample_mt['calcua_alignment_location_fAstCal1.2'].values,
        crams = expand(f"{alignment_dir}/{{study_name}}/{{sequence_id}}/{ref_name}/{{sequence_id}}.mem.crumble.cram", zip, study_name = studies, sequence_id = samples),
        index = expand(f"{alignment_dir}/{{study_name}}/{{sequence_id}}/{ref_name}/{{sequence_id}}.mem.crumble.cram.crai", zip, study_name = studies, sequence_id = samples),
        ref = ref_base + ref_ext
    output:
        bcf= ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites_{chrom}.{chunk}.bcf",
        tot_dp= ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/total_coverage_{chrom}.{chunk}.txt"
    threads: 2
    resources:
        mem_gb=lambda wildcards, threads: threads*2,
        walltime=72
    params:
        regions = lambda wildcards: wildcards.chrom + ":{}-{}".format(*chunk_to_region[int(wildcards.chunk)])
        #regions = lambda wildcards: wildcards.chrom + (f":{START}-{END}" if TEST else "")
    shell:
        """
        bcftools mpileup -a FORMAT/AD,FORMAT/DP -d 3000 --threads {threads} -Ou \
            --regions {params.regions} \
            -f {input.ref} \
            {input.crams} \
        | bcftools call --threads {threads} -f GQ -m -Ou \
        | bcftools +fill-tags --threads {threads} -Ob \
         | tee {output.bcf} | bcftools query -f '[%DP\t]\n'  \
         | awk '{{for(i=1; i<=NF;i++) dp_tot[i]+=$i}} END{{for(i=1; i<=NF;i++) {{printf dp_tot[i]"\\t"}}; printf "\\n"}}' \
          > {output.tot_dp}; \
        bcftools index {output.bcf}
        """



def get_total_dp_files(wildcards):
    cov_files = []
    for chrom in CHROMOSOMES:
        for chunk in get_chunks(chrom):
            fn = ana_dir + '/_data/VariantCalling/{}/{}/'.format(wildcards.ref_name, wildcards.callset_id) + f"total_coverage_{chrom}.{chunk}.txt"
            cov_files.append(fn)
    return cov_files

def write_dp(fn, samples, dps):
    with open(fn,'w') as f:
        for sample, dp in zip(samples, dps):
            f.write(sample + '\t' + str(dp) + '\n')

def read_dp(fn):
    #load mean depth dictionary
    dp_dic = {}
    with open(fn) as f:
        for line in f.readlines():
            sample, dp = line.strip().split()
            dp_dic.update({sample: float(dp)})
    return dp_dic



rule get_mean_coverage:
    input:
        total_dps = get_total_dp_files
    output:
        cov = ana_dir + '/_data/' +  "VariantCalling/{ref_name}/{callset_id}/mean_coverage.txt"
    run:
        mean_dps = [0 for i in range(n_samples)]
        for fn in input.total_dps:
            with open(fn) as f:
                for i, d in enumerate(f.readline().strip().split()):
                    mean_dps[i] += float(d)
        mean_dps = np.array(mean_dps) / (chrom_length.loc[CHROMOSOMES].sum() if not TEST else N_TEST_CHUNKS*chunk_size*len(CHROMOSOMES))
        write_dp(output.cov, samples, mean_dps)



bins = {'MQ': np.linspace(0,61,62),
                'DP': None, # will be set based on mean_dp below
                'MQ0F': np.linspace(0,1,101),
                'MQSB': np.linspace(0,1,101),
                'DD': np.linspace(0,10,401),
                'ExcHetOrig': np.linspace(0,1,101),
                'AB_Het': np.linspace(0,100,401),
                'MF': None } # will be set based on sample_number below




rule min_max_dp:
    input:
        cov = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/mean_coverage.txt"
    output:
        min_dp = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/low_coverage.{ind_filter_id}.txt",
        max_dp = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/excess_coverage.{ind_filter_id}.txt"
    params:
        filter_thresh =lambda wildcards: config['individual_filter_sets'][wildcards.ind_filter_id],
    run:
        mean_dp_dic = read_dp(input.cov)
        #calculate dictionaries for min/max coverage
        # per sample based on Poisson dist and pvals defined under params
        min_dp = []
        max_dp = []
        for dp in mean_dp_dic.values():
            #get the depth thresholds corresponding to <> ind_dp_to_missing_pval of low/excessice coverage
            min_dp.append(stats.poisson.ppf(params.filter_thresh['min_dp_to_missing_pval'], dp))
            max_dp.append(stats.poisson.isf(params.filter_thresh['max_dp_to_missing_pval'], dp))
        write_dp(output.min_dp, mean_dp_dic.keys(), min_dp)
        write_dp(output.max_dp,mean_dp_dic.keys(), max_dp)


rule get_filter_stats:
    """
    This rule calculates filter stats and
    sets individual genotypes to missing according
    thresholds defined in the config file.
    """
    input:
        bcf =  ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites_{chrom}.{chunk}.bcf",
        cov =  ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/mean_coverage.txt",
        min_dp = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/low_coverage.{ind_filter_id}.txt",
        max_dp = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/excess_coverage.{ind_filter_id}.txt"
    output:
        bcf =  ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{chrom}.{chunk}.bcf",
        filter_per_sample  =  ana_dir + '/_data/' + 'VariantCalling/{ref_name}/{callset_id}/filter_per_sample.{ind_filter_id}.{chrom}.{chunk}.tsv',
        total_per_sample =  ana_dir + '/_data/' + 'VariantCalling/{ref_name}/{callset_id}/total_per_sample.{ind_filter_id}.{chrom}.{chunk}.tsv',
        stat_hists =  ana_dir + '/_data/' +'VariantCalling/{ref_name}/{callset_id}/stat_hists.{ind_filter_id}.{chrom}.{chunk}.yaml',
        individual_dp_hists =  ana_dir + '/_data/' + 'VariantCalling/{ref_name}/{callset_id}/individual_dp_hists.{ind_filter_id}.{chrom}.{chunk}.yaml'
        #**{stat:f'{stat}_{{chrom}}.npy' for stat in bins.keys()},
        #**{sample:f'dp_{sample}_{{chrom}}.npy' for sample in samples}
    threads: 2
    resources:
        mem_gb = lambda wildcards, threads: threads*2,
        walltime = 24 #might need to be higher for large samples
    params:
        filter_thresh=lambda wildcards: config['individual_filter_sets'][wildcards.ind_filter_id],
        flush_interval = 1e5,
        #regions = lambda wildcards: ((wildcards.chrom, START, END) if TEST else [])
        regions = lambda wildcards: (wildcards.chrom, chunk_to_region[int(wildcards.chunk)][0],
                                     chunk_to_region[int(wildcards.chunk)][1] )
    run:
        def flush_data():
            for stat, b in bins.items():
                dt = np.array(temp_data[stat])
                dt[dt > b[-1]] = b[-1]
                dt[dt < b[0]] = b[0]
                h, _ = np.histogram(dt,bins=b)
                hists[stat] += h
                temp_data[stat] = []
            for sample, b in dp_per_sample_bins.items():
                dt = np.array(dp_per_sample_tmp[sample])
                dt[dt > b[-1]] = b[-1]
                dt[dt < b[0]] = b[0]
                h, _ = np.histogram(dt,bins=b)
                dp_per_sample_hists[sample] += h
                dp_per_sample_tmp[sample] = []


        #input file stream
        bcf_in = pysam.VariantFile(input.bcf)  # auto-detect input format
        samples1 = [s for s in bcf_in.header.samples]
        for i, (s0, s1) in enumerate(zip(samples, samples1)):
            assert s0==s1,  ("Sample at position {} in metadata and vcf are not the same."
                                        "Metadata {} != VCF {}. That can lead to unexpected behaviour.".format(i,s0, s1))

        n_samples1 = len(samples)
        assert n_samples == n_samples1, "Number of samples in metadata ({}) and VCF ({}) not the same:".format(n_samples, n_samples1)
        bins['MF'] = np.linspace(0, 1, n_samples + 1)

        #this is an output stream to recalculate AN, AC etc after setting some GTs to ./. below
        filltag_stream = subprocess.Popen(['bcftools +fill-tags -Ob'], stdin=subprocess.PIPE, stdout=open(output.bcf, 'w'), stderr=subprocess.PIPE,shell=True, encoding='utf8')


        #load mean depth dictionary
        mean_dp_dic = read_dp(input.cov)
#         mean_dp_dic2 = {} 
#         mean_dp_dic_empty = {}
#         samples_nempty = []
#         samples_empty = []
#         for s in samples:
#             if mean_dp_dic[s] > 0:
#                 mean_dp_dic2[s] =+ mean_dp_dic[s]
#                 samples_nempty.append(s)
#             else:
#                 mean_dp_dic_empty[s] =+ mean_dp_dic[s]
#                 samples_empty.append(s)
        total_dp = np.sum([v for v in mean_dp_dic.values()])
        bins['DP'] = np.arange(0,4 * total_dp, 1)

        #calculate dictionaries for min/max coverage
        # per sample based on Poisson dist and pvals defined under params
        min_dp_dic = read_dp(input.min_dp)
        max_dp_dic = read_dp(input.max_dp)

        #new info annotations calculated below added to vcf header
        bcf_in.header.info.add('DD',1,'Float', 'Average across samples of deviation from individual mean coverage in units of standard deviation.')
        bcf_in.header.info.add('ExcHetOrig','A','Float', 'Excess Heterozygosity before genotype filtering.')
        bcf_in.header.info.add('AB_Het',1,'Float','Phred-scaled p-value of binomial test for allele balance violation in heterozygous genotypes. Larger is worse.')
        bcf_in.header.info.add('MF',1,'Float', 'Fraction of missing (./.) genotypes after genotype filtering.')

        #output object, note how this is piped into filltag_stream for recalcuating INFO tags
        bcf_out = pysam.VariantFile(filltag_stream.stdin,'wu', header=bcf_in.header)

        #filter stats
        filter_per_sample = {s: {'allele_balance': 0,
                                 'low_depth': 0,
                                 'high_depth': 0,
                                 'low_gq': 0} for s in samples}
        total_per_sample = copy.deepcopy(filter_per_sample)


        # the below defines objects to store indoividual depth values
        # and calculate and sum histograms each flush interval
        dp_per_sample_tmp = {s: [] for s in samples}
        dp_per_sample_bins = {s: np.arange(0,5 * mean_dp_dic[s],1) for s in samples}
        dp_per_sample_hists = {k: np.zeros(len(v) - 1) for k, v in dp_per_sample_bins.items()}

        # the below defines objects to store INFO column statistics per line
        # and calculate and sum histograms each flush interval

        hists = {k: np.zeros(len(v) - 1) for k, v in bins.items()}
        temp_data = {k: [] for k in bins.keys()}

        for i, rec in enumerate(bcf_in.fetch(*params.regions)):
            deviations = []
            ads_hets = np.array([0, 0])
            missing = 0
            for n, s in rec.samples.items():
                dp = s['DP']
                if (dp <= min_dp_dic[n]):
                    s['GT'] = (None, None)
                    filter_per_sample[n]['low_depth'] += 1
                total_per_sample[n]['low_depth'] += 1

                if (dp >= max_dp_dic[n]):
                    s['GT'] = (None, None)
                    filter_per_sample[n]['high_depth'] += 1
                total_per_sample[n]['high_depth'] += 1

                #test for het
                if s['GT'][0] != s['GT'][1]:
                    ad = np.array(s['AD'])[list(s['GT'])]
                    ads_hets += ad
                    ab_pval = stats.binom_test(ad)
                    if ab_pval <= params.filter_thresh['allele_balance']:
                        s['GT'] = (None, None)
                        filter_per_sample[n]['allele_balance'] += 1
                    total_per_sample[n]['allele_balance'] += 1

                deviation = np.abs(s['DP'] - mean_dp_dic[n]) / np.sqrt(mean_dp_dic[n])
                deviations.append(deviation)
                try:
                    if s['GQ'] < params.filter_thresh['low_gq']:
                        s['GT'] = (None, None)
                        filter_per_sample[n]['low_gq']+= 1
                    total_per_sample[n]['low_gq'] += 1
                except KeyError:
                    pass

                #add to individual depth list
                dp_per_sample_tmp[n].append(s['DP'])
                if s['GT'] == (None, None):
                    missing += 1
            missing_fraction = missing * 1. / n_samples
            rec.info.update({'MF': missing_fraction})

            #this calculated the mean across individuals of standardised DP deviation
            #large value means that at this site many individuals have a coverage
            #very different from their mean coverage
            mean_deviation = np.mean(deviations)
            rec.info.update({'DD': mean_deviation})

            #if there are any hets, calculate Allele Balance across all samples
            if np.any(ads_hets):
                ab_pval = stats.binom_test(ads_hets)
                ab_phred = -10 * np.log10(ab_pval)
                rec.info.update({'AB_Het': ab_phred})
                rec.info.update({'ExcHetOrig': rec.info['ExcHet']})

            for stat, l in temp_data.items():
                try:
                    st = rec.info[stat]

                    if stat == 'ExcHetOrig':
                        st = st[0]
                    if st is not None:
                        l.append(st)
                except KeyError:
                    pass

            if not (i + 1) % params.flush_interval:
                flush_data()
            #write the line to the output stream
            bcf_out.write(rec)
        #add stats to histogram for last interval
        flush_data()

        stat_dic = {}
        for stat, h in hists.items():
            b = bins[stat]
            stat_dic.update({stat:
                                 {'bins': [float(b0) for b0 in b],
                                  'hists': [int(i) for i in h]}})
        yaml.dump(stat_dic, open(output.stat_hists,'w'))

        d = {}
        for sample, h in dp_per_sample_hists.items():
            b = dp_per_sample_bins[sample]
            d.update({sample:
                          {'bins': [float(b0) for b0 in b],
                          'hists': [int(i) for i in h]}})
        yaml.dump(d, open(output.individual_dp_hists,'w'))

        filter_per_sample_df = pd.DataFrame(filter_per_sample)
        filter_per_sample_df.index.name = 'statistic'
        filter_per_sample_df.columns.name = 'sample'
        filter_per_sample_df.to_csv(output.filter_per_sample,sep='\t')

        total_per_sample_df = pd.DataFrame(total_per_sample)
        total_per_sample_df.index.name = 'statistic'
        total_per_sample_df.columns.name = 'sample'
        total_per_sample_df.to_csv(output.total_per_sample, sep='\t')

        bcf_out.close()
        o,e = filltag_stream.communicate()
        if o is not None:
            print(o,file=sys.stdout)
        if e is not None:
            print(e, file=sys.stderr)

                    
rule plot_individual_filter_stats:
    """
    Plots staistics of indiviudal
    genotypes set to ./. because of
    filter thresholds.
    Also plots individual DP distributions
    """
    input:
        filter_per_samples = [ f'{ana_dir}/_data/VariantCalling/{{ref_name}}/{{callset_id}}/filter_per_sample.{{ind_filter_id}}.{chrom}.{chunk}.tsv' \
                                    for chrom in CHROMOSOMES for chunk in get_chunks(chrom)] ,
        total_per_samples = [f'{ana_dir}/_data/VariantCalling/{{ref_name}}/{{callset_id}}/total_per_sample.{{ind_filter_id}}.{chrom}.{chunk}.tsv' \
                                    for chrom in CHROMOSOMES for chunk in get_chunks(chrom)] ,
        individual_dp_hists = [ f'{ana_dir}/_data/VariantCalling/{{ref_name}}/{{callset_id}}/individual_dp_hists.{{ind_filter_id}}.{chrom}.{chunk}.yaml' \
                                    for chrom in CHROMOSOMES for chunk in get_chunks(chrom)],
        min_dp = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/low_coverage.{ind_filter_id}.txt",
        max_dp = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/excess_coverage.{ind_filter_id}.txt"
    output:
        ind_dp_hist_pdf =  ana_dir + '/Figures/' +'individual_dp_hist.{ref_name}.{callset_id}.{ind_filter_id}.pdf',
        ind_dp_hist_svg =  ana_dir + '/Figures/' +'individual_dp_hist.{ref_name}.{callset_id}.{ind_filter_id}.svg',
        filter_per_sample_pdf =  ana_dir + '/Figures/' + 'filter_per_sample.{ref_name}.{callset_id}.{ind_filter_id}.pdf',
        filter_per_sample_svg = ana_dir + '/Figures/' + 'filter_per_sample.{ref_name}.{callset_id}.{ind_filter_id}.svg',
    #params:
    #    filter_thresh = lambda wildcards: config['individual_filter_sets'][wildcards.ind_filter_id],
    run:
        #Bar plots of how many genotypes per sample were set to mission because
        #of each of the filters
        filtered = pd.read_csv(input.filter_per_samples[0], sep='\t', index_col=0)
        total = pd.read_csv(input.total_per_samples[0],sep='\t',index_col=0)

        for fn1, fn2 in zip(input.filter_per_samples[1:], input.total_per_samples[1:]):
            f = pd.read_csv(fn1, sep='\t', index_col=0)
            t = pd.read_csv(fn2, sep='\t', index_col=0)
            filtered += f
            total += t
        rel_filter = filtered / total

        n_cols = 1
        n_rows = int(np.ceil(len(filtered) / n_cols))
        fig = plt.figure(figsize=(0.2 * n_samples, 4 * n_rows))
        for i, (stat, f) in enumerate(rel_filter.iterrows()):
            ax = fig.add_subplot(n_rows,n_cols,i + 1)
            f.plot(kind='bar',ax=ax,legend=False,label='Total')
            ax.set_title(stat)
            ax.set_ylabel("Proportion filtered")
        plt.tight_layout()

        plt.savefig(output.filter_per_sample_pdf, bbox_inches='tight')
        plt.savefig(output.filter_per_sample_svg, bbox_inches='tight')

        #dictionaries for min/max coverage
        # per sample based on Poisson dist and pvals defined under params
        min_dp_dic = read_dp(input.min_dp)
        max_dp_dic = read_dp(input.max_dp)
        with open(input.individual_dp_hists[0]) as f:
            sd = yaml.safe_load(f)
            bins = {s:v['bins'] for s,v in sd.items()}
            hists = {s:np.array(v['hists']).astype(int) for s,v in sd.items()}
        for fn in input.individual_dp_hists[1:]:
            with open(fn) as f:
                sd = yaml.safe_load(f)
                bins1 = {s: v['bins'] for s, v in sd.items()}
                hists1 = {s: np.array(v['hists']).astype(int) for s, v in sd.items()}
                assert bins1 == bins
                for s,h in hists1.items():
                    hists[s] += h
        bins = {s:np.array(v) for s,v in bins.items()}

        fig = plt.figure(figsize=(16,5*int(np.ceil(len(hists) / 3))))
        for i,(sample, h) in enumerate(hists.items()):
            ax = fig.add_subplot(np.ceil(len(hists) / 3), 3, i+1)
            #poss = bins[sample][:-1] + (bins[sample][1:] - bins[sample][:-1]) / 2
            plt.bar(bins[sample][:-1], h, width=bins[sample][1:] - bins[sample][:-1], align='edge')
            mn = min_dp_dic[sample]
            mx = max_dp_dic[sample]
            plt.axvspan(bins[sample][0], mn, color='r', alpha=0.2)
            plt.axvspan(mx, bins[sample][-1], color='r', alpha=0.2)
            ax.set_xlabel(sample)
        plt.savefig(output.ind_dp_hist_pdf, bbox_inches='tight')
        plt.savefig(output.ind_dp_hist_svg, bbox_inches='tight')



rule plot_site_filter_stats:
    """
    Plots filter statistic distributions with 
    site filter threshold.
    And creates a dynamic site filters to translate
    relative filters into absolute values. 
    """
    input:
        stat_hists = [ f'{ana_dir}/_data/VariantCalling/{{ref_name}}/{{callset_id}}/stat_hists.{{ind_filter_id}}.{chrom}.{chunk}.yaml' \
                            for chrom in CHROMOSOMES for chunk in get_chunks(chrom)],
    output:
        #dynamic_filter_config = f'{ana_dir}/_data/{{ref_name}}/{{callset_id}}/stat_hists.{{ind_filter_id}}.{{site_filter_id}}.yaml',
        stat_hist_pdf =  ana_dir + '/Figures/' +'stat_hists.{ref_name}.{callset_id}.{ind_filter_id}.{site_filter_id}.pdf',
        stat_hist_svg =  ana_dir + '/Figures/' +'stat_hists.{ref_name}}.{callset_id}.{ind_filter_id}.{site_filter_id}.svg',
    params:
        filters = lambda wildcards: config['site_filter_sets'][wildcards.site_filter_id],
    run:
        ##
        with open(input.stat_hists[0]) as f:
            sd = yaml.safe_load(f)
            bins = {s:v['bins'] for s,v in sd.items()}
            hists = {s:np.array(v['hists']).astype(int) for s,v in sd.items()}
        for fn in input.stat_hists[1:]:
            with open(fn) as f:
                sd = yaml.safe_load(f)
                bins1 = {s: v['bins'] for s, v in sd.items()}
                hists1 = {s: np.array(v['hists']).astype(int) for s, v in sd.items()}
                assert bins1 == bins
                for s,h in hists1.items():
                    hists[s] += h

        bins = {s:np.array(v) for s,v in bins.items()}
        n_rows = int(np.ceil(len(params.filters) / 2))
        fig = plt.figure(figsize=(16,5*n_rows))

        for i, (filter_name, fdic) in enumerate(params.filters.items()):
            ax = fig.add_subplot(n_rows, 2, i+1)
            k = fdic['tag']
            try:
                h = hists[k]
            except KeyError as e:
                print("No tag data recorded for filter in config {filter_name} with tag {k}."\
                      "You need add this to the bin dict in rule get_filter_stats.")
            plt.bar(bins[k][:-1],h, width=bins[k][1:] - bins[k][:-1], align='edge')

            assert fdic['threshold_type'] == 'absolute', 'relative threshold not yet implemented'
            if fdic['operator'] == '>':
                mn1 = fdic['threshold']
                mx1 = bins[k][-1]
                #pct_filtered = 100 * (ss > threshold).mean()
            elif fdic['operator'] == '<':
                mn1 = bins[k][0]
                mx1 = fdic['threshold']
                #pct_filtered = 100 * (ss < threshold).mean()
            else:
                raise ValueError("Only < and > operators implemented for filters.")
            plt.axvspan(mn1, mx1, color='r', alpha=0.2)
            ax.set_xlabel(k)
            ax.set_title(filter_name)
        plt.savefig(output.stat_hist_pdf, bbox_inches='tight')
        plt.savefig(output.stat_hist_svg, bbox_inches='tight')

        
######################## apply filters ################################### 


# def get_filter_command(wildcards):
#    commands = []
#    filters = config['site_filter_sets'][wildcards.filter_id]
#
#    for stattype, filter_dic in filter_set.items():
#        filters = filter_dic['filters']
#        dfn = f"{kearad_dir}/analyses/snakemake/Test_bcftools_variantcalling_20210222/VariantCalling/{callset_id}/dynamic_filter_config_{stattype}_{filter_id}.yaml"
#
#        with open(dfn) as f:
#            dyn_filter_dic = yaml.load(f)
#        filters.update(dyn_filter_dic)
#
#        for i, (filter_name, fd) in enumerate(filters.items()):
#            assert fd['threshold_type'] == 'absolute', ("At this point filter thresholds need to be absolute. Check whether dynamic filter config is correctly produced.")
#            commands.append(f"bcftools filter --soft-filter {filter_name} --mode + -O u --exclude '{fd['tag']} {fd['operator']} {fd['threshold']}' ")
#            #-O u: output-type uncompressed BCF
#            #--mode +: append (instead of overwrite (x)) filters
#
#    return(" | ".join(commands))


def get_filter_command(wildcards):
    commands = []
    filters = config['site_filter_sets'][wildcards.site_filter_id]['filters']
    for i, (filter_name, fd) in enumerate(filters.items()):
        assert fd['threshold_type'] == 'absolute', (
            "At this point filter thresholds need to be absolute. Relative thresholds currently not supported.")
        commands.append(
            f"bcftools filter --soft-filter {filter_name} --mode + -O u --exclude '{fd['tag']} {fd['operator']} {fd['threshold']}' ")
    #-O u: output-type uncompressed BCF
    #--mode +: append (instead of overwrite (x)) filters

    return (" | ".join(commands))

rule vcf_add_filters:
    input:
        bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{chrom}.{chunk}.bcf",
    output:
        bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{chrom}.{chunk}.bcf",
    params:
        filter_command = get_filter_command
    shell:
        """
        bcftools view -O u {input.bcf} | \
        {params.filter_command} | \
        bcftools view -O b > {output.bcf}
        """

rule get_filter_info:
    input:
        bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{chrom}.{chunk}.bcf",
    output:
        filter_info = temp(ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{chrom}.{chunk}.filters.info"),
    shell:
        ("""bcftools query --include 'FILTER != "PASS"' """
        "  --format '%CHROM\\t%POS\\t%POS\\t%FILTER\\n' {input.bcf} "
        """ | awk 'BEGIN{{OFS="\t"}} {{print $1,$2,$4}}' > {output.filter_info}  """)


#As discussed with the ERGA pilot committee, Belgium might have ONT sequencing (depending on DNA quality, expected 60-120 Gb output) for one additional species available. This would be sufficient for a ~1 Gb genome. We could use this for another country who does not have funds available. We can discuss today.

rule get_variant_bcf:
    input:
         bcfs = lambda wildcards: expand(ana_dir + '/_data/' + "VariantCalling/{{ref_name}}/{{callset_id}}/all_sites.{{ind_filter_id}}.{{site_filter_id}}.{{chrom}}.{chunk}.bcf",
                    chunk=get_chunks(wildcards.chrom)),
    output:
        bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.variants.{chrom}.bcf",
        bcf_ix = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.variants.{chrom}.bcf.csi",
    shell:
        """
        bcftools concat -n -Ou {input.bcfs} | bcftools view -Ob --exclude 'ALT = "."'   > {output.bcf};
        bcftools index -f {output.bcf}
        """


rule merge_chrom_filter_files:
    input:
        filter_infos = lambda wildcards: expand(ana_dir + '/_data/' + "VariantCalling/{{ref_name}}/{{callset_id}}/all_sites.{{ind_filter_id}}.{{site_filter_id}}.{{chrom}}.{chunk}.filters.info",
                                                 chunk=get_chunks(wildcards.chrom)),
    output:
        filter_info = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{chrom}.filters.info.gz",
        filter_bed = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{chrom}.filters.bed.gz",
        filter_info_ix = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{chrom}.filters.info.gz.tbi",
        filter_bed_ix = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{chrom}.filters.bed.gz.tbi",
    params:
        merge_dist=lambda wildcards: config['site_filter_sets'][wildcards.site_filter_id]["merge_dist"]
    shell:
        """
        cat {input.filter_infos} | tee >(bgzip -c > {output.filter_info}) \
        | awk  'BEGIN{{OFS="\\t"}} {{print $1,$2-1,$2}}' \
        | bedtools merge -d {params.merge_dist} | bgzip -c  >  {output.filter_bed}; 
         tabix -s 1 -b 2 {output.filter_info}; 
         tabix -p bed {output.filter_bed}; 
        """

rule select_biallelic_pass_snps:
    input:
         bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.variants.{chrom}.bcf",
    output:
        vcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{chrom}.bcf",
        index1 = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{chrom}.bcf.tbi",
        index2 = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{chrom}.bcf.csi",
#         vcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.NoSFilter.snps.biallelic.{chrom}.bcf",
#         index1 = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.NoSFilter.snps.biallelic.{chrom}.bcf.tbi",
#         index2 = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.NoSFilter.snps.biallelic.{chrom}.bcf.csi",
    shell:
        """
        bcftools view \
            -Oz \
            --max-alleles 2 \
            --types snps \
            --apply-filters PASS \
            {input.bcf} > {output.vcf};
         bcftools index -f {output.vcf};
         bcftools index --tbi -f {output.vcf}
        """
#         bcftools view \
#             -Oz \
#             --max-alleles 2 \
#             --types snps \
#             {input.bcf} > {output.vcf};
#          bcftools index -f {output.vcf};
#          bcftools index --tbi -f {output.vcf}
#          """

# rule concat_vcfs:
#     input:
#         vcfs = expand("{{vcf}}.chr{chrom}.vcf.gz", chrom=CHROMOSOMES)
#     output:
#         vcf = "{vcf}.all_chroms.vcf.gz",
#         index1 =  "{vcf}.all_chroms.vcf.gz.tbi",
#         index2 = "{vcf}.all_chroms.vcf.gz.csi"
#     shell:
#         """
#         bcftools concat -Oz {input.vcfs} > {output.vcf}
#         bcftools index -f {output.vcf}
#         bcftools index --tbi -f {output.vcf}
#         """
        

# rule concat_bcfs:
#     input:
#         vcfs = expand("{{vcf}}.chr{chrom}.vcf.gz", chrom=CHROMOSOMES)
#     output:
#         bcf = "{vcf}.all_chroms.bcf",
#         index1 =  "{vcf}.all_chroms.bcf.tbi",
#         index2 = "{vcf}.all_chroms.bcf.csi"
#     shell:
#         """
#         bcftools concat -Oz {input.vcfs} > {output.bcf}
#         bcftools index -f {output.bcf}
#         bcftools index --tbi -f {output.bcf}
#         """


######################## create bedfiles ################################### 


def get_interval_df(chrom, pos):
    """
    Get a .bed-like interval data frame
    from an array of postions pos.
    chrom can be single chromosomes or list/array.
    Careful! This returns BED-like
    0-indexed right open intervals,
    while the input is assumed 1-indexed
    VCF/SAM-like.
    """
    pos = np.array(pos)
    diff = np.diff(pos)
    start_vals = np.insert(diff,0,[2])
    end_vals =  np.append(diff,[2])
    starts = pos[start_vals>1]
    ends = pos[end_vals>1] + 1 #bed right open
    interval_df = pd.DataFrame({'start':starts-1, #BED-like 0-indexed
                                'end':ends-1,
                                'chrom':chrom})
    return interval_df.loc[:,['chrom','start','end']]


rule get_site_filter_mask:
    input:
        fai = ref_base + ref_ext + '.fai',
        vcf_site_stats = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.site_stats.{{chrom}}.tsv.gz",
        dynamic_filter_config = f"{ana_dir}/_data/VariantCalling/{callset_id}/dynamic_filter_config_site_{filter_id}.yaml",
    output:
        anycov_bed = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.non_zero_coverage.{filter_id}.{{chrom}}.bed"),
        chrom_bed = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.chromosome.{filter_id}.{{chrom}}.bed"),
        nocov_bed = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.no_coverage.{filter_id}.{{chrom}}.bed"),
        site_filters_graph = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.all_site_filters.{filter_id}.{{chrom}}.bedgraph"),
        site_filters_merged = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.site_filters_merged.{filter_id}.{{chrom}}.bed"),
        accessible_bed = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.accessible_site_filters.{filter_id}.{{chrom}}.bed")
    run:
        
        chrom_len = pd.read_csv(input.fai, sep='\t',index_col=0, usecols=[0,1],names=['chrom','length'], squeeze=True).loc[wildcards.chrom]
        site_stat_df = pd.read_csv(input.vcf_site_stats, sep='\t',na_values=['.','NA','NaN'], index_col=[0,1])
        
        filters = config['filter_sets'][filter_id]['site']['filters']
        with open(input.dynamic_filter_config) as f:
            dyn_filter_dic = yaml.load(f)
        filters.update(dyn_filter_dic)

        #filter_beds
        filter_fns = []
        for filter_name, d in  filters.items():
            assert d['threshold_type'] == 'absolute', ("At this point filter thresholds need to be absolute. Check whether dynamic filter config is correctly produced.")

            ss = site_stat_df.loc[:,d['tag']]
            ss = ss[ss.apply(is_float)].astype(float)
            sel_str = "ss {} {}".format(d['operator'], d['threshold'])
            bed_df = get_interval_df(wildcards.chrom, ss[eval(sel_str)].index.get_level_values(1))
            bed_df.loc[:,'description'] = filter_name

            fn = jn(kearad_dir,'analyses/_data/VariantCalling',callset_id, f'{callset_id}.{filter_name}.{filter_id}.{wildcards.chrom}.bed')
            bed_df.to_csv(fn, sep='\t', header=False, index=False)
            filter_fns.append(fn)
        
        #nocov_bed
        any_cov = get_interval_df(wildcards.chrom, site_stat_df['DP'].dropna().index.get_level_values(1))
        any_cov.loc[:,'description'] = 'NonZeroCoverage'
        any_cov.to_csv(output.anycov_bed, sep='\t', header=False, index=False)

        all_chrom = pd.DataFrame({'chrom': wildcards.chrom, 'start': [0], 'end': [chrom_len]}, index=[0]).loc[:,['chrom','start','end']]
        all_chrom.loc[:,'description'] = 'Chromosome'
        all_chrom.to_csv(output.chrom_bed, sep='\t', header=False, index=False)

        shell("""bedtools subtract -a {output.chrom_bed} -b {output.anycov_bed}"""
""" | awk 'BEGIN{{OFS="\t"}} {{$4="NoCov";print$0}}' > {output.nocov_bed}""")
        
        #combine filter and novoc beds -> inaccessible and accessible bed files
        filter_fns_str = " ".join(filter_fns)
        shell("bedtools unionbedg -i {output.nocov_bed} {filter_fns_str} > {output.site_filters_graph}")
        shell("""bedtools merge -i {output.site_filters_graph}"""
              """ | awk 'BEGIN{{OFS="\t"}} {{$4="SiteFilters";print$0}}' > {output.site_filters_merged}""")
        shell("""bedtools subtract -a {output.chrom_bed} -b {output.site_filters_merged}"""
              """ | awk 'BEGIN{{OFS="\t"}} {{$4="Accessible";print$0}}' > {output.accessible_bed}""")
        
        for fn in filter_fns:
            os.remove(fn)

            
rule get_snp_filter_mask:
    input:
        vcf = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.variants.{filter_id}.{{chrom}}.bcf"
    output:
        snp_filters_graph = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.all_snp_filters.{filter_id}.{{chrom}}.bedgraph"),
        snp_filters_merged = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.snp_filters_merged.{filter_id}.{{chrom}}.bed"),
    run:
        filters = config['filter_sets'][filter_id]['snp']['filters']
        filter_fns = []
        for i, (filter_name, fd) in enumerate(filters.items()):
            fn = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.{filter_name}.{filter_id}.{wildcards.chrom}.bed"
            shell(f"""bcftools query --include '(TYPE="snp")&(FILTER ~ "{filter_name}")' --format '%CHROM\\t%POS\\t%FILTER\\n' {{input.vcf}}"""
                """ | awk 'BEGIN{{OFS="\\t"}} {{print $1,$2-1,$2,"{filter_name}"}}' """
                f""" | uniq >  {fn}""")
            filter_fns.append(fn)
        
        filter_fn_str = " ".join(filter_fns)
        shell(f"""bedtools unionbedg -i {filter_fn_str} > {{output.snp_filters_graph}}""")
        shell("""bedtools merge -i {output.snp_filters_graph}"""
              """ | awk 'BEGIN{{OFS="\t"}} {{$4="VariantFilters";print$0}}' > {output.snp_filters_merged}""")

        for fn in filter_fns:
            os.remove(fn)           
                        

rule combine_site_snp_filters:
    input:
        snp_filters_merged = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.snp_filters_merged.{filter_id}.{{chrom}}.bed",
        site_filters_merged = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.site_filters_merged.{filter_id}.{{chrom}}.bed",
        accessible_sites_bed = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.accessible_site_filters.{filter_id}.{{chrom}}.bed",
    output:\
        filters_merged = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.all_filters_merged.{filter_id}.{{chrom}}.bed"),
        accessible_bed = temp(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.accessible_site_and_snp.{filter_id}.{{chrom}}.bed")
    shell:
        """
        bedtools unionbedg -i {input.site_filters_merged} {input.snp_filters_merged} | \
        bedtools merge -i stdin > {output.filters_merged}
        bedtools subtract \
            -a {input.accessible_sites_bed} \
            -b {output.filters_merged} \
            > {output.accessible_bed}
        """


rule get_all_chrom_bed_masks:
    input:
        snp_filters_merged = expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.snp_filters_merged.{filter_id}.{{chrom}}.bed", chrom=CHROMOSOMES),
        site_filters_merged = expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.site_filters_merged.{filter_id}.{{chrom}}.bed", chrom=CHROMOSOMES),
        filters_merged = expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.all_filters_merged.{filter_id}.{{chrom}}.bed", chrom=CHROMOSOMES),
        snp_filters_graph = expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.all_snp_filters.{filter_id}.{{chrom}}.bedgraph", chrom=CHROMOSOMES),
        site_filters_graph = expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.all_site_filters.{filter_id}.{{chrom}}.bedgraph", chrom=CHROMOSOMES),
        accessible_site_bed = expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.accessible_site_filters.{filter_id}.{{chrom}}.bed", chrom=CHROMOSOMES),
        accessible_bed = expand(f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.accessible_site_and_snp.{filter_id}.{{chrom}}.bed", chrom=CHROMOSOMES),
    output:\
        snp_filters_all_chrom = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.snp_filters_merged.{filter_id}.all_chrom.bed.gz",
        site_filters_all_chrom = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.site_filters_merged.{filter_id}.all_chrom.bed.gz",
        all_filters_all_chrom = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.site_and_snp_filters_merged.{filter_id}.all_chrom.bed.gz",
        snp_filters_graph_all_chrom = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.snp_filters.{filter_id}.all_chrom.bedgraph.gz",
        site_filters_graph_all_chrom = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.site_filters.{filter_id}.all_chrom.bedgraph.gz",
        accessible_site_all_chrom = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.accessible_site.{filter_id}.all_chrom.bed.gz",
        accessible_site_and_snp_all_chrom = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.accessible_site_and_snp.{filter_id}.all_chrom.bed.gz",
        accessible_size = f"{ana_dir}/_data/VariantCalling/{callset_id}/{callset_id}.{filter_id}.accessible_size.txt"
    shell:
        """
        cat {input.snp_filters_merged} | gzip -c > {output.snp_filters_all_chrom}
        cat {input.site_filters_merged} | gzip -c > {output.site_filters_all_chrom}
        cat {input.filters_merged} | gzip -c > {output.all_filters_all_chrom}
        cat {input.snp_filters_graph} | gzip -c > {output.snp_filters_graph_all_chrom}
        cat {input.site_filters_graph} | gzip -c > {output.site_filters_graph_all_chrom}
        cat {input.accessible_site_bed} | gzip -c > {output.accessible_site_all_chrom}
        cat {input.accessible_bed} | gzip -c > {output.accessible_site_and_snp_all_chrom}
        
        printf "accessible\t" > {output.accessible_size}
        zcat {output.accessible_site_and_snp_all_chrom} | \
            awk 'BEGIN{{tot=0}} {{tot=tot+$3-$2}} END{{print tot}}' \
            >> {output.accessible_size}
        
        printf "filtered\t" >> {output.accessible_size}
        zcat {output.all_filters_all_chrom} | \
            awk 'BEGIN{{tot=0}} {{tot=tot+$3-$2}} END{{print tot}}' \
            >> {output.accessible_size}
        """

######################## phasing whatshap ###################################

#def get_bams(samples):
#    return sample_mt.loc[samples, "calcua_alignment_location_fAstCal1.2"].tolist()


#phasing all individuals at the same time works in principle but is too slow

#
# rule phase_whatshap:
#     input:
#         vcf = ana_dir + '/_data/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.{chrom}.vcf.gz",
#         bams = get_bams(SAMPLES),
#         ref= ref_base + ref_ext
#     output:
#         bcf = ana_dir + '/_data/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.whatshap.{chrom}.bcf",
#         csi = ana_dir + '/_data/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.pass.snps.biallelic.whatshap.{chrom}.bcf.csi"
#     conda:
#         "phasing_env.yaml"
#     threads: 1
#     resources:
#         mem_gb= lambda wildcards, attempt: int(np.sqrt(attempt) * 18),
#         walltime=lambda wildcards, attempt: attempt
#     shell:
#         """
#         whatshap phase --tag PS --reference={input.ref} {input.vcf} {input.bams} | bcftools view -Ob > {output.bcf}
#         bcftools index -f {output.bcf}
#         """
        
## PS = Phase Set
## HP = compatibel met GATK
## GT = genotype
## => where the phasing info is written to

def get_bam(sample):
    return sample_mt.loc[sample, "calcua_alignment_location_fAstCal1.2"]

def get_crams(sequence_id):
    study_name = sample_mt.loc[sequence_id,'study']
    return f"{alignment_dir}/{study_name}/{sequence_id}/{{ref_name}}/{sequence_id}.mem.crumble.cram"

#beagle

#running, but weird sm error when running other rules. 

# rule phase_whatshap:
#     input:
#         bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.{chrom}.bcf",
#         bcf_ix = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.{chrom}.bcf.csi",
#         bam = lambda wildcards: get_crams(wildcards.sample_id),
#         ref= ref_base + ref_ext
#     output:
#         bcf = temp(ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{chrom}.{sample_id}.bcf"),
#         csi = temp(ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{chrom}.{sample_id}.bcf.csi"),
#     conda:
#         "phasing_env.yaml"
#     threads: 1
#     resources:
#         mem_gb= lambda wildcards, attempt: int(np.sqrt(attempt) * 18),
#         walltime= 12 #lambda wildcards, attempt: attempt
#     params:
#         tmp_vcf = lambda wildcards, output: output.bcf.rsplit('.',1)[0] + '.tmp.vcf'
#     shell:
#         """
#         bcftools view --samples {wildcards.sample_id} --regions {wildcards.chrom} {input.bcf} > {params.tmp_vcf}
#         echo "begin whatshap"
#         whatshap phase --tag PS --reference={input.ref} {params.tmp_vcf} {input.bam} | bcftools view -Ob > {output.bcf}
#         echo "finished whatshap"
#         rm {params.tmp_vcf};
#         echo "indexing"
#         bcftools index -f {output.bcf}
#         """


# rule combine_whatshap:
#     #group: "whatshap"
#     input:
#         bcfs = expand(ana_dir + '/_data/' + "VariantCalling/{{ref_name}}/{{callset_id}}/all_sites.{{ind_filter_id}}.{{site_filter_id}}.{{vcf_type}}.whatshap.{{chrom}}.{sample_id}.bcf",
#                       sample_id=samples),
#     output:
#         bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{chrom}.bcf",
#         index = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{chrom}.bcf.csi"
#     threads: 1
#     resources:
#         mem_gb=lambda wildcards, threads: threads*4,
#         walltime=12
#     shell:
#         ("bcftools merge -Ob  --info-rules '-' {input.bcfs} > {output.bcf} ; "
#         " bcftools index -f {output.bcf}")


######################## phasing shapeit4 ###################################
##only running on biomina, can't submit the job
##but working from here on:

# rule phase_shapeit:
#     input:
#         bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{chrom}.bcf",
#         index = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{chrom}.bcf.csi",
#     output:
#         #bcf = temp(ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.{chrom}.{chunk}.bcf"),
#         bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.{chrom}.bcf",
#         index = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.{chrom}.bcf.csi"
#     threads: 6 #10 on hopper
#     resources:
#         mem_gb= 24, #lambda wildcards, threads: threads*4, #120gb on hopper
#         walltime= 24
#     conda:
#         "envs/phasing_env_2.yaml"
#     #params:
#         #regions = lambda wildcards: wildcards.chrom + ":{}-{}".format(*chunk_to_region[int(wildcards.chunk)])
#     shell:
#         """
#         shapeit4 --thread {threads} --use-PS 0.0001 --region {wildcards.chrom} --input {input.bcf}  --output {output.bcf} --sequencing
#         bcftools index -f --threads 7 {output.bcf}
#         """
#         # shapeit4 --thread {threads} --use-PS 0.0001 --region {params.regions}  --input {input.bcf}  --output {output.bcf} --sequencing
#         # """

# rule concat_phased_vcf:
#     input:
#         bcfs = lambda wildcards: expand(ana_dir + '/_data/' + "VariantCalling/{{ref_name}}/{{callset_id}}/all_sites.{{ind_filter_id}}.{{site_filter_id}}.{{vcf_type}}.whatshap.shapeit4.{{chrom}}.{chunk}.bcf",
#                 chunk=get_chunks(wildcards.chrom)),
#     output:
#         bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.{chrom}.bcf",
#         index = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.{chrom}.bcf.csi"
#     shell:
#         """
#         bcftools concat -n -Ou -o {output.bcf} {input.bcfs}
#         bcftools index -f --threads 7 {output.bcf}
#         """


# ###################### annotate phasing ###################################


# rule annotate_phase_vcf:
#     input:
#         phase_vcf = f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.{{chrom}}.bcf',
#         vcf = f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{{chrom}}.bcf',
#         vcf_ix = f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.{{chrom}}.bcf.csi'
#     output:
#         vcf = f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.{{chrom}}.vcf.gz',
#         index1 = f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.{{chrom}}.vcf.gz.csi',
#         index2 = f'{ana_dir}/_data/VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.{{chrom}}.vcf.gz.tbi',
#     resources:
#         walltime=1
#     shell:
#         """
#         bcftools annotate -c INFO,FORMAT,FILTER -a {input.phase_vcf} {input.vcf} | bcftools +fill-AN-AC -O z > {output.vcf}
#         bcftools index -f {output.vcf}
#         bcftools index -f --tbi {output.vcf}
#         """


# rule concat_phased_anno_vcf:
#     input:
#         bcfs = expand(ana_dir + '/_data/' + "VariantCalling/{{ref_name}}/{{callset_id}}/all_sites.{{ind_filter_id}}.{{site_filter_id}}.{{vcf_type}}.whatshap.shapeit4.annotate.{chrom}.vcf.gz",
#                 chrom = CHROMOSOMES),
#     output:
#         bcf = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.wg.bcf",
#         index = ana_dir + '/_data/' + "VariantCalling/{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.wg.bcf.csi"
#     resources:
#         walltime=24
#     shell:
#         """
#         bcftools concat -n -Ou -o {output.bcf} {input.bcfs}
#         bcftools index -f --threads 7 {output.bcf}
#         """

######################## ancestral_sample ###################################           

# the regex syntax "(a|b|c)" gives a discrete choice between either a,b or c.
# you can extend the choices if you have other vcf types you want to annotate with
# ancestral state
def get_output_type(wildcards):
    x = wildcards.vcf_extension
    if x == 'bcf':
        return 'b'
    elif x == 'vcf.gz':
        return 'z'
    elif x == 'vcf':
        return 'u'
    else:
        raise UserException("Unknown vcf_extension {}".format(x))



rule add_ancestral_fasta_as_sample:
    input:
        vcf = ana_dir + '/_data/VariantCalling/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.{chrom}.vcf.gz",
        anc_fasta = "/scratch/antwerpen/grp/asvardal/projects/cichlid/reference/fAstCal1.2/ancestral_sequence/anc_all_chrs.AstCalCypFro_final.PNsequence.deletionsAsN.fa",
#         vcf = ana_dir + '/_data/VariantCalling/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.{chrom}.bcf",
    output:
        vcf = ana_dir + '/_data/VariantCalling/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.anc_samp.{chrom}.{vcf_extension}",
        index1 = ana_dir + '/_data/VariantCalling/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.anc_samp.{chrom}.{vcf_extension}.tbi",
        index2 = ana_dir + '/_data/VariantCalling/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.whatshap.shapeit4.annotate.anc_samp.{chrom}.{vcf_extension}.csi"
#         vcf = ana_dir + '/_data/VariantCalling/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.anc_samp.{chrom}.{vcf_extension}",
#         index1 = ana_dir + '/_data/VariantCalling/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.anc_samp.{chrom}.{vcf_extension}.tbi",
#         index2 = ana_dir + '/_data/VariantCalling/' + "{ref_name}/{callset_id}/all_sites.{ind_filter_id}.{site_filter_id}.{vcf_type}.anc_samp.{chrom}.{vcf_extension}.csi"
    params:
        output_type = get_output_type,
        ancestral_name = 'ancestral'
    resources:
        mem_gb=14,#lambda wildcards, threads: threads*2,
        walltime = 24
    run:
        from pyfasta import Fasta
        import re
        anc_seq = Fasta(input.anc_fasta)
        read_bcf_stream = subprocess.Popen(['bcftools view {}'.format(input.vcf)],
                                             stdout=subprocess.PIPE,
                                             stderr=subprocess.PIPE,shell=True,
                                             encoding='utf8')

        #potentially compress with bcftools to get an indexable vcf.gz
        gzip_stream = subprocess.Popen(['bcftools view -O{}'.format(params.output_type)],
                                        stdin=subprocess.PIPE,
                                        stdout=open(output.vcf,'w'),
                                        stderr=subprocess.PIPE,shell=True,
                                        encoding='utf8')
        #with open(gzip_stream.stdin,'w') as f:
        n_rec = 0
        for line in read_bcf_stream.stdout:
            if not line.startswith('#'):
                n_rec += 1
                sline = line.strip().split()
                chrom1 = sline[0]
                pos = int(sline[1])
                ref = sline[3]
                alts = sline[4].split(',')
                #print(ref, alts)
                anc = anc_seq[chrom1][pos - 1]
                if anc == ref:
                    genotype = '0/0'
                elif anc in alts:
                    for i, alt in enumerate(alts):
                        if anc == alt:
                            genotype = '{}/{}'.format(i + 1,i + 1)
                else:
                    genotype = './.'
                #in order to match all the GQ, DP, AD, .. annotation format use the ones from the last sample but replace with 0s
                new_gt = genotype + re.sub('[0-9]+','0',sline[-1][3:])
                line = '\t'.join(sline + [new_gt]) + '\n'
            elif line.startswith('#CHROM'):
                line = line.strip() + '\t' + params.ancestral_name + '\n'
            #print(line)
            gzip_stream.stdin.write(line)
        #wait till all the file writing operations are finished
        _,e = gzip_stream.communicate()
        if e:
            raise UserException(e)
        shell("bcftools index -f {output.vcf}")
        shell("bcftools index --tbi -f {output.vcf}")


