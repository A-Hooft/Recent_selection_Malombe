#use conda enviroment hscon5
shell.prefix("source /data/antwerpen/grp/asvardal/share/hscon5_setup.sh; ")

#packages 
import os, gzip
import itertools
import pandas as pd
import random
import numpy as np
jn = os.path.join

configfile: "config.yaml"
wildcard_constraints:

ana_dir = config['ana_dir']
data_dir = config['data_dir']

########################################################
## metadata file defined in the config file under "sample_mt".
sample_mt = pd.read_csv(config["sample_mt"], dtype=str, sep='\t', index_col=0)#.set_index("sequence_id", drop=False)

## can be run with multiple window sizes so that you can compare results
## trade off between noise and specificity 
## step is the "overlap" between windows, so window all SNPs in 50,000kb window will be read twice if step is half the window size. 
## to run perSNP just set window and step to 1

# windows = [1, 10000, 50000, 100000]
# steps = [1, 5000, 25000, 50000]
window = 1
step = 1

##change this for naming run, i use the convention P1--P2-P3
# trios = 'Malombe--Nkhudzi-SE_arm' #arangment of trio

trios = 'Nkhudzi--Malombe-SE_arm' #arangment of trio

species = 'virginalis' #species or group being investigated
region_id = 'wg' #for whole genome


## create a list of sample ids from metadata table
samples = list(sample_mt.index.values)
## if you want to subset your metadata table do this as follows:
#samples = list(sample_mt[sample_mt['species'] == species].index.values)
#samples = list(sample_mt[(sample_mt['species'] == "virginalis")&(sample_mt['location'] == "Lake_Malombe") | (sample_mt['location'] == "Nkhudzi_Bay")].index.values)

########################################################
# three functions to create the different groups, I group populations by "location" in my metadata, but any value is fine. 
def write_grp1(wildcards):
    with open(f"{ana_dir}/output/fst/grp1.txt",'w') as f:
        for sequence_id in samples:
            x = sample_mt.loc[sequence_id,'location']
            if x == 'Nkhudzi_Bay': # this is 
                f.write(sequence_id + '\n')

def write_grp2(wildcards):
    with open(f"{ana_dir}/output/fst/grp2.txt",'w') as f:
        for sequence_id in samples:
            x = sample_mt.loc[sequence_id,'location']
            if x in "Lake_Malombe":
                f.write(sequence_id + '\n')
            
def write_grp3(wildcards):
    with open(f"{ana_dir}/output/fst/grp3.txt",'w') as f:
        for sequence_id in samples:
            x = sample_mt.loc[sequence_id,'location']
            if x == 'Southeast_arm':
                f.write(sequence_id + '\n')

## function to read fst file correctly
def read_pop_fst(fn):
    fst_df = pd.read_csv(fn,sep='\t',index_col=[0,1]).dropna().squeeze("columns")
    fst_df.index = pd.MultiIndex.from_arrays([fst_df.index.droplevel(1),
                                                 (fst_df.index.droplevel(0)+(fst_df['BIN_END']-fst_df.index.droplevel(0))/2).astype(int)])
    return fst_df

def read_pop_fst2(fn):
    fst_df = pd.read_csv(fn,sep='\t',index_col=[0,1]).fillna(0).squeeze("columns")
    return fst_df

########################################################
rule all:
    input:
        f"{ana_dir}/output/fst/p12.{region_id}.windowed.weir.fst",
        f"{ana_dir}/output/fst/p13.{region_id}.windowed.weir.fst",
        f"{ana_dir}/output/fst/p23.{region_id}.windowed.weir.fst",
        f"{ana_dir}/output/fst/PBS_{species}.{trios}.{region_id}.windowed.weir.fst"

         
rule windowed_fst_for_pbs_12:
    input:
        vcf = f"{data_dir}/all_sites.tif1.sft1.pass.snps.biallelic.whatshap.shapeit4.annotate.anc_samp.wg.vcf.gz"
    output:
        p12 = f"{ana_dir}/output/fst/p12.{region_id}.windowed.weir.fst"
    params:
        prefix_out_12 = f"{ana_dir}/output/fst/p12.{region_id}",
        fn_grp1 = f"{ana_dir}/output/fst/grp1.txt",
        fn_grp2 = f"{ana_dir}/output/fst/grp2.txt",
        fn_grp3 = f"{ana_dir}/output/fst/grp3.txt",
    resources:
        walltime=12
    run:
        write_grp1(wildcards)
        write_grp2(wildcards)
        shell("vcftools --gzvcf {input.vcf} --weir-fst-pop {params.fn_grp1} --weir-fst-pop {params.fn_grp2} --out {params.prefix_out_12} --fst-window-size {window} --fst-window-step {step}")

rule windowed_fst_for_pbs_13:
    input:
        vcf = f"{data_dir}/all_sites.tif1.sft1.pass.snps.biallelic.whatshap.shapeit4.annotate.anc_samp.wg.vcf.gz"
    output:
        p13 =  f"{ana_dir}/output/fst/p13.{region_id}.windowed.weir.fst"
    params:
        prefix_out_13 = f"{ana_dir}/output/fst/p13.{region_id}",
        fn_grp1 = f"{ana_dir}/output/fst/grp1.txt",
        fn_grp2 = f"{ana_dir}/output/fst/grp2.txt",
        fn_grp3 = f"{ana_dir}/output/fst/grp3.txt",
    resources:
        walltime=12
    run:
        write_grp1(wildcards)
        write_grp3(wildcards),
        shell("vcftools --gzvcf {input.vcf} --weir-fst-pop {params.fn_grp1} --weir-fst-pop {params.fn_grp3} --out {params.prefix_out_13} --fst-window-size {window} --fst-window-step {step}")

rule windowed_fst_for_pbs_23:
    input:
        vcf = f"{data_dir}/all_sites.tif1.sft1.pass.snps.biallelic.whatshap.shapeit4.annotate.anc_samp.wg.vcf.gz"
    output:
        p23 = f"{ana_dir}/output/fst/p23.{region_id}.windowed.weir.fst"
    params:
        prefix_out_23 = f"{ana_dir}/output/fst/p23.{region_id}",
        fn_grp1 = f"{ana_dir}/output/fst/grp1.txt",
        fn_grp2 = f"{ana_dir}/output/fst/grp2.txt",
        fn_grp3 = f"{ana_dir}/output/fst/grp3.txt",
    resources:
        walltime=12
    run:
        write_grp2(wildcards)
        write_grp3(wildcards),
        shell("vcftools --gzvcf {input.vcf} --weir-fst-pop {params.fn_grp2} --weir-fst-pop {params.fn_grp3} --out {params.prefix_out_23} --fst-window-size {window} --fst-window-step {step}")


rule windowed_fst_for_pbs_concat:
    input:
        p12 = f"{ana_dir}/output/fst/p12.{region_id}.windowed.weir.fst",
        p13 = f"{ana_dir}/output/fst/p13.{region_id}.windowed.weir.fst",
        p23 = f"{ana_dir}/output/fst/p23.{region_id}.windowed.weir.fst"
    output:
        out = f"{ana_dir}/output/fst/PBS_{species}.{trios}.{region_id}.windowed.weir.fst"
    run:
        fst12 = read_pop_fst(input.p12)
        fst13 = read_pop_fst(input.p13)
        fst23 = read_pop_fst(input.p23)

        #!!! maybe logical if you want get rid of windows that have few variants.
#         fst12 = fst12[fst12['N_VARIANTS']>50]
#         fst13 = fst13[fst13['N_VARIANTS']>50]
#         fst23 = fst23[fst23['N_VARIANTS']>50]

        fst12.rename({'BIN_END': '12_BIN_END', 'N_VARIANTS': '12_N_VARIANTS','WEIGHTED_FST': '12_WEIGHTED_FST', 'WEIGHTED_FST': '12_WEIGHTED_FST', 'MEAN_FST': '12_MEAN_FST'}, axis=1, inplace=True)
        fst13.rename({'BIN_END': '13_BIN_END', 'N_VARIANTS': '13_N_VARIANTS','WEIGHTED_FST': '13_WEIGHTED_FST', 'WEIGHTED_FST': '13_WEIGHTED_FST', 'MEAN_FST': '13_MEAN_FST'}, axis=1, inplace=True)
        fst23.rename({'BIN_END': '23_BIN_END', 'N_VARIANTS': '23_N_VARIANTS','WEIGHTED_FST': '23_WEIGHTED_FST', 'WEIGHTED_FST': '23_WEIGHTED_FST', 'MEAN_FST': '23_MEAN_FST'}, axis=1, inplace=True)

        pw_3_pop = pd.concat([fst12, fst13, fst23], axis=1, join = 'inner')
        f = lambda x: -np.log(1-x)
        PBS_mean_pop1 = 0.5*(f(pw_3_pop['12_MEAN_FST'])+f(pw_3_pop['13_MEAN_FST'])-f(pw_3_pop['23_MEAN_FST']))
        pw_3_pop['PBS_mean_fst_pop1'] = PBS_mean_pop1
        PBS_weight_pop1 = 0.5*(f(pw_3_pop['12_WEIGHTED_FST'])+f(pw_3_pop['13_WEIGHTED_FST'])-f(pw_3_pop['23_WEIGHTED_FST']))
        pw_3_pop['PBS_weighted_fst_pop1'] = PBS_weight_pop1

        pw_3_pop.to_csv(output.out, sep="\t")
        
        